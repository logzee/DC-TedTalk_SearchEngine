import pandas as pd
import numpy as np
import string
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.stem import SnowballStemmer
from nltk import WordNetLemmatizer

# data importing
data = pd.read_csv("ted talk data - Sheet1.csv")
data = data.iloc[0:5, :]

# --- DATA PRE-PROCESSING ---
# removing stopwords & punctuation
stopWords = stopwords.words()
rowIndex = 0
for transcript in data.iloc[:, 0]:
    new_transcript = ""
    if type(transcript) == float:
        break
    transcript = np.str.lower(transcript)
    transcript = np.str.replace(transcript, "'", "")
    text_tokens = word_tokenize(transcript)
    for word in text_tokens:
        if word not in string.punctuation:
            if word not in stopWords:
                new_transcript = new_transcript + " " + word
    new_transcript = ("").join(new_transcript)
    data.iloc[rowIndex, 0] = new_transcript
    rowIndex += 1


# Stemming & Lemmatizing
sbStemmer = SnowballStemmer('english')
def stemSentence(sentence):
    token_words = word_tokenize(sentence)
    stem_sentence = []
    for word in token_words:
        stem_sentence.append(sbStemmer.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)


wordnet_lemmatizer = WordNetLemmatizer()
rowIndex = 0

for transcript in data.iloc[:, 0]:
    new_transcript = ""
    for word in transcript:
        lemWord = wordnet_lemmatizer.lemmatize(word)
        new_transcript = new_transcript + lemWord
    new_transcript = stemSentence(new_transcript)
    data.iloc[rowIndex, 0] = new_transcript
    rowIndex += 1
print(data)

# feature extraction

# sentence input


## Create Vocabulary
vocabulary = set()
for doc in data:
    vocabulary.update(doc.split(','))
vocabulary = list(vocabulary)
# Intializating the tfIdf model
tfidf = TfidfVectorizer(vocabulary=vocabulary)
# Fit the TfIdf model
tfidf.fit(data)
# Transform the TfIdf model
tfidf_tran=tfidf.transform(data)

def gen_vector_T(tokens):
    Q = np.zeros((len(vocabulary)))
    x= tfidf.transform(tokens)
    #print(tokens[0].split(','))
    for token in tokens[0].split(','):
        #print(token)
        try:
            ind = vocabulary.index(token)
            Q[ind]  = x[0, tfidf.vocabulary_[token]]
        except:
            pass
    return Q

def cosine_sim(a, b):
    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))
    return cos_sim


def cosine_similarity_T(k, query):
    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()
    tokens = word_tokenize(str(preprocessed_query))
    q_df = pd.DataFrame(columns=['q_clean'])
    q_df.loc[0, 'q_clean'] = tokens
    q_df['q_clean'] = wordnet_lemmatizer.lemmatize(q_df.q_clean)
    d_cosines = []

    query_vector = gen_vector_T(q_df['q_clean'])
    for d in tfidf_tran.A:
        d_cosines.append(cosine_sim(query_vector, d))

    out = np.array(d_cosines).argsort()[-k:][::-1]
    # print("")
    d_cosines.sort()
    a = pd.DataFrame()
    for i, index in enumerate(out):
        a.loc[i, 'index'] = str(index)
        a.loc[i, 'Subject'] = data['Subject'][index]
    for j, simScore in enumerate(d_cosines[-k:][::-1]):
        a.loc[j, 'Score'] = simScore
    return a

cosine_similarity_T(10,"computer science")


